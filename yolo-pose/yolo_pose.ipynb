{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f46d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viplab/miniconda3/envs/videomae2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt to 'yolo11n-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.97M/5.97M [00:00<00:00, 115MB/s]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO(\"yolo11m-pose.pt\")  # load an official model\n",
    "model = YOLO(\"yolo11n-pose.pt\")  # load a custom model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c92266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /media/viplab/Storage1/driver_action_recognition/crop_new/cut_frames_rear/class_0/user_id_53307/Rear_view_user_id_53307_7_4471.jpg: 640x640 1 person, 25.7ms\n",
      "Speed: 6.6ms preprocess, 25.7ms inference, 28.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# results = model(\"/media/viplab/Storage1/driver_action_recognition/crop_new/cut_frames_rear/class_9/user_id_36305/Rear_view_user_id_36305_3_8947.jpg\")  # predict on an image\n",
    "results = model(\"/media/viplab/Storage1/driver_action_recognition/crop_new/cut_frames_rear/class_0/user_id_53307/Rear_view_user_id_53307_7_4471.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360a3916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[9.9526e-01, 7.3902e-01, 9.9733e-01, 4.2246e-03, 9.9589e-01, 9.9431e-01, 9.8879e-01, 8.4267e-01, 7.1015e-01, 7.8587e-01, 7.3971e-01, 3.4196e-01, 1.9581e-01, 1.6393e-02, 8.2240e-03, 2.0659e-03, 8.8357e-04]], device='cuda:0')\n",
      "data: tensor([[[2.5988e+02, 1.4759e+02, 9.9526e-01],\n",
      "         [2.6646e+02, 1.2474e+02, 7.3902e-01],\n",
      "         [2.3473e+02, 1.2515e+02, 9.9733e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2246e-03],\n",
      "         [1.7464e+02, 1.3259e+02, 9.9589e-01],\n",
      "         [2.6667e+02, 2.5636e+02, 9.9431e-01],\n",
      "         [9.9956e+01, 2.6102e+02, 9.8879e-01],\n",
      "         [2.8178e+02, 4.3712e+02, 8.4267e-01],\n",
      "         [1.0696e+02, 4.6254e+02, 7.1015e-01],\n",
      "         [3.1402e+02, 4.1957e+02, 7.8587e-01],\n",
      "         [2.6757e+02, 4.6585e+02, 7.3971e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4196e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9581e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6393e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 8.2240e-03],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0659e-03],\n",
      "         [0.0000e+00, 0.0000e+00, 8.8357e-04]]], device='cuda:0')\n",
      "has_visible: True\n",
      "orig_shape: (512, 512)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[259.8792, 147.5935],\n",
      "         [266.4579, 124.7436],\n",
      "         [234.7299, 125.1534],\n",
      "         [  0.0000,   0.0000],\n",
      "         [174.6398, 132.5944],\n",
      "         [266.6721, 256.3631],\n",
      "         [ 99.9557, 261.0250],\n",
      "         [281.7800, 437.1182],\n",
      "         [106.9594, 462.5372],\n",
      "         [314.0201, 419.5738],\n",
      "         [267.5680, 465.8550],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000]]], device='cuda:0')\n",
      "xyn: tensor([[[0.5076, 0.2883],\n",
      "         [0.5204, 0.2436],\n",
      "         [0.4585, 0.2444],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3411, 0.2590],\n",
      "         [0.5208, 0.5007],\n",
      "         [0.1952, 0.5098],\n",
      "         [0.5504, 0.8537],\n",
      "         [0.2089, 0.9034],\n",
      "         [0.6133, 0.8195],\n",
      "         [0.5226, 0.9099],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    print(result.keypoints)\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict with the model\n",
    "# results = model(\"/media/viplab/Storage1/driver_action_recognition/crop_new/cut_frames_side/class_9/user_id_36305/Right_side_window_user_id_36305_3_8947.jpg\")  # predict on an image\n",
    "results = model.track(source=\"/media/viplab/DATADRIVE1/driver_action_recognition/crop_videos/train/user_id_13522_5/Rear_view_user_id_13522_NoAudio_5.MP4\", save=True, show=True)  # predict on an image\n",
    "\n",
    "# Access the results\n",
    "for result in results:\n",
    "    xy = result.keypoints.xy  # x and y coordinates\n",
    "    xyn = result.keypoints.xyn  # normalized\n",
    "    kpts = result.keypoints.data  # x, y, visibility (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b46ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /media/viplab/Storage1/driver_action_recognition/crop_new/cut_frames_side/class_9/user_id_36305/Right_side_window_user_id_36305_3_8947.jpg: 640x640 1 person, 19.0ms\n",
      "Speed: 17.3ms preprocess, 19.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "results = model(\"/media/viplab/Storage1/driver_action_recognition/crop_new/cut_frames_side/class_9/user_id_36305/Right_side_window_user_id_36305_3_8947.jpg\")  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68f18e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4160, 0.6211],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.4131, 0.6010],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.3435, 0.5717],\n",
       "        [0.2013, 0.5055],\n",
       "        [0.2903, 0.7027],\n",
       "        [0.2019, 0.4385],\n",
       "        [0.3405, 0.9382],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].keypoints.xyn[0][:11]\n",
    "\n",
    "# final_keypoints = results[0].keypoints[highest_conf_idx].xyn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ff9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[187.9316, 143.3364],\n",
      "        [  0.0000,   0.0000],\n",
      "        [175.0820, 131.1202],\n",
      "        [  0.0000,   0.0000],\n",
      "        [139.6472, 145.3846],\n",
      "        [ 96.2096, 186.2070],\n",
      "        [157.3366, 232.7977],\n",
      "        [192.8087, 160.0798],\n",
      "        [247.3005, 313.5802],\n",
      "        [256.6579, 130.8009],\n",
      "        [289.0013, 207.6659],\n",
      "        [ 93.0947, 415.6727],\n",
      "        [134.3389, 432.9201],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000]], device='cuda:0')\n",
      "Keypoint vector shape: torch.Size([34])\n",
      "[tensor([187.9316, 143.3364,   0.0000,   0.0000, 175.0820, 131.1202,   0.0000,   0.0000, 139.6472, 145.3846,  96.2096, 186.2070, 157.3366, 232.7977, 192.8087, 160.0798, 247.3005, 313.5802, 256.6579, 130.8009, 289.0013, 207.6659,  93.0947, 415.6727, 134.3389, 432.9201,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for result in results:\n",
    "    keypoints = result.keypoints\n",
    "\n",
    "    kpts_array = keypoints.xy if hasattr(keypoints, 'xy') else keypoints.data.cpu().numpy()\n",
    "\n",
    "    # print(kpts_array)\n",
    "    # If the keypoints object doesn't have 'xy', try 'data' attribute or convert accordingly\n",
    "    # For example, if keypoints is a torch tensor:\n",
    "    # kpts_array = keypoints.cpu().numpy()\n",
    "\n",
    "    # Flatten keypoints per instance: (num_keypoints * 3)\n",
    "    # If you want only (x, y), slice [:, :, :2]\n",
    "    keypoint_vectors = []\n",
    "    for instance_kpts in kpts_array:\n",
    "        print(instance_kpts)\n",
    "        # Flatten (num_keypoints, 3) -> (num_keypoints*3,)\n",
    "        flattened = instance_kpts.flatten()\n",
    "        keypoint_vectors.append(flattened)\n",
    "\n",
    "    # Now keypoint_vectors is a list of 1D arrays, one per detected person/object\n",
    "    # If you have multiple detections, you can select one or aggregate\n",
    "\n",
    "    # Example: Use the first detected person's keypoints as input\n",
    "    if len(keypoint_vectors) > 0:\n",
    "        input_keypoints = keypoint_vectors[0]  # 1D vector for first detected person\n",
    "        print(\"Keypoint vector shape:\", input_keypoints.shape)\n",
    "        # You can now concatenate this with your ResNet features for Mamba input\n",
    "    else:\n",
    "        print(\"No keypoints detected in this frame.\")\n",
    "    \n",
    "print(keypoint_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6e8a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e63e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[187.9316, 143.3364],\n",
      "        [  0.0000,   0.0000],\n",
      "        [175.0820, 131.1202],\n",
      "        [  0.0000,   0.0000],\n",
      "        [139.6472, 145.3846],\n",
      "        [ 96.2096, 186.2070],\n",
      "        [157.3366, 232.7977],\n",
      "        [192.8087, 160.0798],\n",
      "        [247.3005, 313.5802],\n",
      "        [256.6579, 130.8009],\n",
      "        [289.0013, 207.6659],\n",
      "        [ 93.0947, 415.6727],\n",
      "        [134.3389, 432.9201],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000]], device='cuda:0')\n",
      "Keypoint vector shape: torch.Size([34])\n"
     ]
    }
   ],
   "source": [
    "keypoints = results[0].keypoints\n",
    "\n",
    "kpts_array = keypoints.xy if hasattr(keypoints, 'xy') else keypoints.data.cpu().numpy()\n",
    "\n",
    "    # print(kpts_array)\n",
    "    # If the keypoints object doesn't have 'xy', try 'data' attribute or convert accordingly\n",
    "    # For example, if keypoints is a torch tensor:\n",
    "    # kpts_array = keypoints.cpu().numpy()\n",
    "\n",
    "    # Flatten keypoints per instance: (num_keypoints * 3)\n",
    "    # If you want only (x, y), slice [:, :, :2]\n",
    "keypoint_vectors = []\n",
    "for instance_kpts in kpts_array:\n",
    "    print(instance_kpts)\n",
    "    # Flatten (num_keypoints, 3) -> (num_keypoints*3,)\n",
    "    flattened = instance_kpts.flatten()\n",
    "    keypoint_vectors.append(flattened)\n",
    "\n",
    "    # Now keypoint_vectors is a list of 1D arrays, one per detected person/object\n",
    "    # If you have multiple detections, you can select one or aggregate\n",
    "\n",
    "    # Example: Use the first detected person's keypoints as input\n",
    "if len(keypoint_vectors) > 0:\n",
    "    input_keypoints = keypoint_vectors[0]  # 1D vector for first detected person\n",
    "    print(\"Keypoint vector shape:\", input_keypoints.shape)\n",
    "\n",
    "       # You can now concatenate this with your ResNet features for Mamba input\n",
    "else:\n",
    "    print(\"No keypoints detected in this frame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b83799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([187.9316, 143.3364,   0.0000,   0.0000, 175.0820, 131.1202,   0.0000,   0.0000, 139.6472, 145.3846,  96.2096, 186.2070, 157.3366, 232.7977, 192.8087, 160.0798, 247.3005, 313.5802, 256.6579, 130.8009, 289.0013, 207.6659,  93.0947, 415.6727, 134.3389, 432.9201,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "keypoints = results[0].keypoints.xy.flatten()\n",
    "\n",
    "print(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce10094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22114ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id_16080_5',\n",
       " 'user_id_16700_5',\n",
       " 'user_id_22640_5',\n",
       " 'user_id_25077_7',\n",
       " 'user_id_25432_7',\n",
       " 'user_id_35418_5',\n",
       " 'user_id_38159_7',\n",
       " 'user_id_38479_5',\n",
       " 'user_id_38479_7',\n",
       " 'user_id_41850_5',\n",
       " 'user_id_42711_7',\n",
       " 'user_id_46141_7',\n",
       " 'user_id_46844_5',\n",
       " 'user_id_47158_5',\n",
       " 'user_id_50347_7',\n",
       " 'user_id_50921_7',\n",
       " 'user_id_52046_5',\n",
       " 'user_id_53307_5',\n",
       " 'user_id_59014_7',\n",
       " 'user_id_59581_5',\n",
       " 'user_id_61962_5',\n",
       " 'user_id_63764_5',\n",
       " 'user_id_63764_7',\n",
       " 'user_id_69039_5',\n",
       " 'user_id_70176_7',\n",
       " 'user_id_71720_7',\n",
       " 'user_id_87903_5',\n",
       " 'user_id_98067_5',\n",
       " 'user_id_99882_7']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/media/viplab/DATADRIVE1/driver_action_recognition/pose_resnet_features/A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c135f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videomae2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
